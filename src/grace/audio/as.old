"""
Grace AI System - Audio System Module

This module integrates the audio components for the Grace AI system.
"""

import logging
import time
import numpy as np
from typing import Dict, Optional, Tuple, Union

# Import audio components
from .audio_input import AudioInput
from .audio_output import AudioOutput
from .speech_recognition import SpeechRecognizer
from .audio_utils import trim_silence, normalize_audio


class AudioSystem:
    """
    Integrated audio system for Grace AI.
    
    This class combines audio input, output, and speech recognition
    to provide a complete audio interface for the Grace AI system.
    """
    
    def __init__(self, config: Dict):
        """
        Initialize the audio system with the provided configuration.
        
        Args:
            config: Audio system configuration
        """
        self.logger = logging.getLogger('grace.audio')
        self.audio_config = config.get('audio', {})
        
        # Initialize components
        self.input = AudioInput(config)
        self.output = AudioOutput(config)
        self.recognizer = SpeechRecognizer(config)
        
        # Flag to track if the system is enabled
        self.enabled = not self.audio_config.get('disable_audio', False)
        
        if not self.enabled:
            self.logger.info("Audio system is disabled")
        else:
            self.logger.info("Audio system initialized")
    
    def start_listening(self) -> bool:
        """
        Start listening for audio input.
        
        Returns:
            Success status
        """
        if not self.enabled:
            return False
            
        return self.input.start_listening()
    
    def stop_listening(self):
        """Stop listening for audio input."""
        if self.enabled:
            self.input.stop_listening()
    
    def listen_and_transcribe(self) -> Tuple[str, Optional[np.ndarray]]:
        """
        Listen for speech and transcribe it.
        
        Returns:
            Tuple of (transcribed_text, audio_data)
        """
        if not self.enabled:
            return "", None
            
        # Listen for command
        audio_data = self.input.listen_for_command()
        if audio_data is None or len(audio_data) == 0:
            return "", None
            
        # Process audio
        if self.audio_config.get('trim_silence', True):
            audio_data = trim_silence(audio_data)
            
        if self.audio_config.get('normalize_audio', True):
            audio_data = normalize_audio(audio_data)
            
        # Transcribe audio
        text = self.recognizer.transcribe(audio_data)
        return text, audio_data
    
    async def listen_and_transcribe_async(self) -> Tuple[str, Optional[np.ndarray]]:
        """
        Listen for speech and transcribe it asynchronously.
        
        Returns:
            Tuple of (transcribed_text, audio_data)
        """
        if not self.enabled:
            return "", None
            
        # Listen for command
        audio_data = self.input.listen_for_command()
        if audio_data is None or len(audio_data) == 0:
            return "", None
            
        # Process audio
        if self.audio_config.get('trim_silence', True):
            audio_data = trim_silence(audio_data)
            
        if self.audio_config.get('normalize_audio', True):
            audio_data = normalize_audio(audio_data)
            
        # Transcribe audio asynchronously
        text = await self.recognizer.transcribe_async(audio_data)
        return text, audio_data
    
    def speak(self, text: str) -> bool:
        """
        Convert text to speech and play it.
        
        Args:
            text: Text to speak
            
        Returns:
            Success status
        """
        if not self.enabled or not text:
            return False
            
        return self.output.speak(text)
    
    async def speak_async(self, text: str) -> bool:
        """
        Convert text to speech and play it asynchronously.
        
        Args:
            text: Text to speak
            
        Returns:
            Success status
        """
        if not self.enabled or not text:
            return False
            
        return await self.output.speak_async(text)
    
    def transcribe_audio(self, audio_data: Union[str, np.ndarray]) -> str:
        """
        Transcribe audio data.
        
        Args:
            audio_data: Audio data (path or numpy array)
            
        Returns:
            Transcribed text
        """
        if not self.enabled:
            return ""
            
        return self.recognizer.transcribe(audio_data)
    
    async def transcribe_audio_async(self, audio_data: Union[str, np.ndarray]) -> str:
        """
        Transcribe audio data asynchronously.
        
        Args:
            audio_data: Audio data (path or numpy array)
            
        Returns:
            Transcribed text
        """
        if not self.enabled:
            return ""
            
        return await self.recognizer.transcribe_async(audio_data)
    
    def get_audio_devices(self) -> list:
        """
        Get a list of available audio devices.
        
        Returns:
            List of audio device information
        """
        from .audio_utils import get_device_list
        return get_device_list()
    
    def get_status(self) -> Dict:
        """
        Get detailed status of the audio system.
        
        Returns:
            Dictionary with status information
        """
        status = {
            "enabled": self.enabled,
            "input": self.input.get_status(),
            "output": self.output.get_status(),
            "recognizer": self.recognizer.get_status()
        }
        
        # Add overall state
        status["input_ready"] = status["input"]["microphone_available"] and not status["input"]["last_mic_error"]
        status["output_ready"] = not status["output"]["last_tts_error"]
        status["recognition_ready"] = status["recognizer"]["whisper_model_loaded"]
        
        status["overall_ready"] = all([
            status["enabled"],
            status["input_ready"],
            status["output_ready"],
            status["recognition_ready"]
        ])
        
        return status
    
    def stop(self):
        """Stop all audio components and clean up resources."""
        self.logger.info("Shutting down audio system")
        
        # Stop all components
        self.input.stop()
        self.output.stop()
        self.recognizer.stop()
