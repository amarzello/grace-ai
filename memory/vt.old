"""
Grace AI System - Vector Storage Module

This module implements vector-based memory storage using mem0ai.
"""

import logging
import asyncio
import json
import time
from typing import Dict, List, Any, Optional, Union
from pathlib import Path

from grace.utils.common import MEMORY_DB_PATH, calculate_relevance
from .types import MemoryType

# Try to import required dependencies
try:
    from mem0 import Memory, AsyncMemory
    MEM0_AVAILABLE = True
except ImportError:
    MEM0_AVAILABLE = False
    # Create comprehensive fallback classes for type checking and basic functionality
    class Memory:
        def __init__(self, **kwargs):
            self.memories = {}
            self.api_key = kwargs.get('api_key')
            self.logger = logging.getLogger('grace.memory.vector.fallback')
            self.logger.warning("Using fallback Memory class - mem0ai not available")
        
        def add(self, content, **kwargs):
            """Add memory in fallback mode."""
            import hashlib
            import time
            
            # Generate a unique ID
            content_hash = hashlib.md5(content.encode()).hexdigest()
            timestamp = int(time.time())
            memory_id = f"fallback_{content_hash}_{timestamp}"
            
            # Store memory
            self.memories[memory_id] = {
                'content': content,
                'user_id': kwargs.get('user_id', 'default'),
                'metadata': kwargs.get('metadata', {})
            }
            
            return {'id': memory_id}
            
        def search(self, query, **kwargs):
            """Search memories in fallback mode."""
            results = []
            
            # Simple search by relevance
            for memory_id, memory in self.memories.items():
                content = memory.get('content', '')
                relevance = calculate_relevance(query, content)
                
                if relevance > 0.2:  # Minimum relevance threshold
                    results.append({
                        'id': memory_id,
                        'content': content,
                        'user_id': memory.get('user_id'),
                        'metadata': memory.get('metadata', {}),
                        'score': relevance
                    })
            
            # Sort by relevance
            results.sort(key=lambda x: x['score'], reverse=True)
            
            # Apply limit if specified
            limit = kwargs.get('limit', 10)
            results = results[:limit]
            
            return {'results': results}
        
        def delete(self, memory_id):
            """Delete memory in fallback mode."""
            if memory_id in self.memories:
                del self.memories[memory_id]
                return True
            return False
            
    class AsyncMemory:
        def __init__(self, **kwargs):
            self.sync_memory = Memory(**kwargs)
            self.logger = logging.getLogger('grace.memory.vector.fallback')
            self.logger.warning("Using fallback AsyncMemory class - mem0ai not available")
            
        async def add(self, content, **kwargs):
            """Add memory asynchronously in fallback mode."""
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(
                None, lambda: self.sync_memory.add(content, **kwargs)
            )
            
        async def search(self, query, **kwargs):
            """Search memories asynchronously in fallback mode."""
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(
                None, self.sync_memory.search, query, **kwargs
            )
            
        async def delete(self, memory_id):
            """Delete memory asynchronously in fallback mode."""
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(
                None, self.sync_memory.delete, memory_id
            )


class VectorStorage:
    """Vector-based storage using mem0ai."""
    
    def __init__(self, config: Dict):
        """
        Initialize vector storage with the provided configuration.
        
        Args:
            config: Configuration dictionary
        """
        self.logger = logging.getLogger('grace.memory.vector')
        self.config = config.get('memory', {})
        self.amnesia_mode = config.get('amnesia_mode', False)
        
        # Initialize mem0ai client
        self._init_vector_store()
        
        # Map to track memory_type by memory ID
        self.memory_type_map = {}
        
    def _init_vector_store(self):
        """Initialize vector store with mem0ai."""
        if not MEM0_AVAILABLE:
            self.logger.error("mem0ai library not available - using fallback vector memory implementation")
            self.mem0 = Memory()
            self.async_mem0 = AsyncMemory()
            self.is_initialized = True
            return
            
        try:
            # Check if API key is configured
            api_key = self.config.get('mem0_api_key')
            if api_key:
                # Use hosted mem0ai service with API key
                self.logger.info("Using mem0ai hosted service with API key")
                # For synchronous operations
                self.mem0 = Memory(api_key=api_key)
                # For asynchronous operations
                self.async_mem0 = AsyncMemory(api_key=api_key)
            else:
                # Use self-hosted/local version without API key
                self.logger.info("Using local mem0ai without API key")
                self.mem0 = Memory()
                self.async_mem0 = AsyncMemory()
                
            self.is_initialized = True
            self.logger.info("Vector storage initialized with mem0ai")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize vector storage: {e}")
            # Fall back to simpler implementation
            self.mem0 = Memory()
            self.async_mem0 = AsyncMemory()
            self.is_initialized = True
            
    async def add_memory(self, content: str, memory_type: MemoryType, metadata: Dict = None) -> Optional[str]:
        """
        Add memory to vector storage.
        
        Args:
            content: Memory content
            memory_type: Type of memory
            metadata: Additional metadata
            
        Returns:
            Vector storage identifier
        """
        if self.amnesia_mode or not self.is_initialized:
            return None
            
        if not content or not content.strip():
            return None
            
        # Prepare metadata
        if metadata is None:
            metadata = {}
            
        metadata['memory_type'] = memory_type.value
        metadata['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # Add to mem0ai
        try:
            # Generate a consistent user_id for this type of memory
            user_id = f"grace_{memory_type.value}"
            
            # Add the memory using the async client
            result = await self.async_mem0.add(
                content,
                user_id=user_id,
                metadata=metadata
            )
            
            # Handle different result formats
            if isinstance(result, dict) and 'id' in result:
                memory_id = result['id']
            elif isinstance(result, str):
                memory_id = result
            else:
                # Generate a deterministic ID based on content if none returned
                import hashlib
                memory_id = hashlib.md5(content.encode()).hexdigest()
                
            # Track memory type
            self.memory_type_map[memory_id] = memory_type.value
            
            return memory_id
                
        except Exception as e:
            self.logger.error(f"Error adding memory to vector storage: {e}")
            return None
            
    async def search_memories(self, query: str, limit: int = 50) -> Dict[str, List[Dict]]:
        """
        Search memories in vector storage.
        
        Args:
            query: Search query
            limit: Maximum results to return
            
        Returns:
            Search results dictionary
        """
        if not self.is_initialized:
            return {"results": []}
            
        try:
            # Define search options
            search_options = {
                "limit": limit,
                "threshold": 0.2  # Minimum relevance threshold
            }
            
            # Search across all user_ids (memory types)
            all_results = []
            memory_types = [m.value for m in MemoryType]
            
            for memory_type in memory_types:
                user_id = f"grace_{memory_type}"
                
                # Create filters to search within this memory type
                filters = {
                    "AND": [
                        {"user_id": user_id}
                    ]
                }
                
                # Perform the search
                results = await self.async_mem0.search(
                    query,
                    filters=filters,
                    **search_options
                )
                
                # Process results based on format
                if isinstance(results, dict) and "results" in results:
                    # Ensure all results have the memory_type in metadata
                    for result in results["results"]:
                        if isinstance(result, dict) and "metadata" in result:
                            if "memory_type" not in result["metadata"]:
                                result["metadata"]["memory_type"] = memory_type
                        elif isinstance(result, dict):
                            result["metadata"] = {"memory_type": memory_type}
                    all_results.extend(results["results"])
                elif isinstance(results, list):
                    # Ensure all results have the memory_type
                    for result in results:
                        if isinstance(result, dict) and "metadata" in result:
                            if "memory_type" not in result["metadata"]:
                                result["metadata"]["memory_type"] = memory_type
                        elif isinstance(result, dict):
                            result["metadata"] = {"memory_type": memory_type}
                    all_results.extend(results)
            
            # Sort by relevance and limit overall results
            all_results.sort(
                key=lambda x: x.get('score', 0) if isinstance(x, dict) else 0, 
                reverse=True
            )
            return {"results": all_results[:limit]}
            
        except Exception as e:
            self.logger.error(f"Vector search error: {e}")
            return {"results": []}
    
    async def delete_memory(self, memory_id: str) -> bool:
        """
        Delete a memory from vector storage.
        
        Args:
            memory_id: Memory ID to delete
            
        Returns:
            Success status
        """
        if not self.is_initialized or self.amnesia_mode:
            return False
            
        try:
            # Delete the memory
            result = await self.async_mem0.delete(memory_id)
            
            # Clean up tracking
            if memory_id in self.memory_type_map:
                del self.memory_type_map[memory_id]
                
            return True
        except Exception as e:
            self.logger.error(f"Error deleting memory from vector storage: {e}")
            return False
            
    async def optimize_indexes(self):
        """Optimize vector indexes for better performance."""
        if not self.is_initialized or self.amnesia_mode:
            return
            
        try:
            # mem0ai API doesn't currently expose index optimization
            # This is a placeholder for future implementation
            
            # We can implement some basic housekeeping:
            # 1. Check for orphaned memories (not in SQLite but in vector store)
            # 2. Verify memory type mappings
            # 3. Reindex if needed
            
            self.logger.info("Vector index optimization is a no-op for mem0ai")
            return True
        except Exception as e:
            self.logger.error(f"Error optimizing vector indexes: {e}")
            return False
            
    async def get_stats(self):
        """
        Get statistics about vector storage.
        
        Args:
            None
            
        Returns:
            Dictionary with statistics
        """
        stats = {
            "initialized": self.is_initialized,
            "mem0_available": MEM0_AVAILABLE,
            "memory_types": {}
        }
        
        # Count memories by type
        for memory_type in self.memory_type_map.values():
            stats["memory_types"][memory_type] = stats["memory_types"].get(memory_type, 0) + 1
            
        # Add total count
        stats["total_memories"] = len(self.memory_type_map)
        
        return stats
        
    def close(self):
        """Clean up resources."""
        self.logger.info("Closing vector storage")
        
        # mem0ai doesn't currently require explicit cleanup
        # This is a placeholder for future implementation
        self.is_initialized = False
